# Named Entity Recognition (NER) Project
# Complete Step-by-Step Implementation

# ============================================================================
# STEP 1: IMPORT LIBRARIES
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# For preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# For deep learning
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import (LSTM, Embedding, Dense, TimeDistributed, 
                                      Dropout, Bidirectional, Input)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

print("Libraries imported successfully!")
print(f"TensorFlow version: {tf.__version__}")


# ============================================================================
# STEP 2: LOAD AND EXPLORE DATA
# ============================================================================

# Load the dataset
df = pd.read_csv('ner_dataset.csv', encoding='latin1')

# Fill NaN values in Sentence # column
df['Sentence #'].fillna(method='ffill', inplace=True)

print("Dataset loaded successfully!")
print(f"\nDataset shape: {df.shape}")
print(f"\nFirst few rows:")
print(df.head(15))

# Basic statistics
print(f"\n{'='*60}")
print("DATASET STATISTICS")
print(f"{'='*60}")
print(f"Total rows: {len(df)}")
print(f"Total unique sentences: {df['Sentence #'].nunique()}")
print(f"Total unique words: {df['Word'].nunique()}")
print(f"Total unique POS tags: {df['POS'].nunique()}")
print(f"Total unique NER tags: {df['Tag'].nunique()}")

# Check for missing values
print(f"\nMissing values:")
print(df.isnull().sum())

# Distribution of NER tags
print(f"\n{'='*60}")
print("NER TAG DISTRIBUTION")
print(f"{'='*60}")
tag_counts = df['Tag'].value_counts()
print(tag_counts)


# ============================================================================
# STEP 3: DATA VISUALIZATION
# ============================================================================

# Visualize tag distribution
plt.figure(figsize=(12, 6))
tag_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of NER Tags', fontsize=16, fontweight='bold')
plt.xlabel('NER Tags', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Sentence length distribution
sentence_lengths = df.groupby('Sentence #').size()
plt.figure(figsize=(12, 6))
plt.hist(sentence_lengths, bins=50, color='coral', edgecolor='black', alpha=0.7)
plt.title('Distribution of Sentence Lengths', fontsize=16, fontweight='bold')
plt.xlabel('Number of Words per Sentence', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nSentence Length Statistics:")
print(f"Mean: {sentence_lengths.mean():.2f}")
print(f"Median: {sentence_lengths.median():.2f}")
print(f"Max: {sentence_lengths.max()}")
print(f"Min: {sentence_lengths.min()}")


# ============================================================================
# STEP 4: DATA PREPROCESSING
# ============================================================================

print(f"\n{'='*60}")
print("DATA PREPROCESSING")
print(f"{'='*60}")

# Group by sentences
class SentenceGetter(object):
    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),
                                                            s['POS'].values.tolist(),
                                                            s['Tag'].values.tolist())]
        self.grouped = self.data.groupby('Sentence #').apply(agg_func)
        self.sentences = [s for s in self.grouped]
    
    def get_next(self):
        try:
            s = self.grouped['Sentence: {}'.format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(df)
sentences = getter.sentences

print(f"Total sentences: {len(sentences)}")
print(f"\nExample sentence:")
print(sentences[0])

# Extract words and tags
X = [[word[0] for word in sentence] for sentence in sentences]
y = [[word[2] for word in sentence] for sentence in sentences]

print(f"\nExample processed:")
print(f"Words: {X[0]}")
print(f"Tags: {y[0]}")

# Get unique words and tags
words = list(set(df['Word'].values))
words.append('ENDPAD')
words.append('UNK')  # For unknown words
n_words = len(words)

tags = list(set(df['Tag'].values))
n_tags = len(tags)

print(f"\nVocabulary size: {n_words}")
print(f"Number of tags: {n_tags}")
print(f"Tags: {tags}")

# Create word and tag dictionaries
word2idx = {w: i + 1 for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}
idx2tag = {i: t for t, i in tag2idx.items()}

print(f"\nTag to index mapping:")
for tag, idx in tag2idx.items():
    print(f"  {tag}: {idx}")


# ============================================================================
# STEP 5: SEQUENCE PREPARATION
# ============================================================================

print(f"\n{'='*60}")
print("SEQUENCE PREPARATION")
print(f"{'='*60}")

# Convert words to indices
X_encoded = [[word2idx.get(w, word2idx['UNK']) for w in s] for s in X]

# Convert tags to indices
y_encoded = [[tag2idx[t] for t in s] for s in y]

# Pad sequences to same length
max_len = 50  # Maximum sequence length
print(f"Maximum sequence length set to: {max_len}")

X_padded = pad_sequences(maxlen=max_len, sequences=X_encoded, 
                         padding='post', value=word2idx['ENDPAD'])
y_padded = pad_sequences(maxlen=max_len, sequences=y_encoded, 
                         padding='post', value=tag2idx['O'])

print(f"\nShape after padding:")
print(f"X_padded: {X_padded.shape}")
print(f"y_padded: {y_padded.shape}")

# One-hot encode the tags
y_categorical = np.array([to_categorical(i, num_classes=n_tags) for i in y_padded])
print(f"y_categorical shape: {y_categorical.shape}")


# ============================================================================
# STEP 6: TRAIN-VALIDATION-TEST SPLIT
# ============================================================================

print(f"\n{'='*60}")
print("DATA SPLITTING")
print(f"{'='*60}")

# First split: separate test set (20%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X_padded, y_categorical, test_size=0.2, random_state=42
)

# Second split: separate train and validation (80-20 of remaining)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, random_state=42
)

print(f"Training set: {X_train.shape[0]} sentences ({X_train.shape[0]/len(X_padded)*100:.1f}%)")
print(f"Validation set: {X_val.shape[0]} sentences ({X_val.shape[0]/len(X_padded)*100:.1f}%)")
print(f"Test set: {X_test.shape[0]} sentences ({X_test.shape[0]/len(X_padded)*100:.1f}%)")


# ============================================================================
# STEP 7: BASELINE MODEL (Simple LSTM)
# ============================================================================

print(f"\n{'='*60}")
print("BUILDING BASELINE MODEL")
print(f"{'='*60}")

# Define baseline model
def create_baseline_model(n_words, n_tags, max_len):
    model = Sequential([
        Embedding(input_dim=n_words + 1, output_dim=50, input_length=max_len),
        LSTM(units=100, return_sequences=True),
        TimeDistributed(Dense(n_tags, activation='softmax'))
    ])
    
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    return model

baseline_model = create_baseline_model(n_words, n_tags, max_len)
print("\nBaseline Model Architecture:")
baseline_model.summary()

# Train baseline model
print("\nTraining baseline model...")
history_baseline = baseline_model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=10,
    validation_data=(X_val, y_val),
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_baseline.history['loss'], label='Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Validation Loss')
plt.title('Baseline Model - Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history_baseline.history['accuracy'], label='Training Accuracy')
plt.plot(history_baseline.history['val_accuracy'], label='Validation Accuracy')
plt.title('Baseline Model - Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()


# ============================================================================
# STEP 8: EVALUATE BASELINE MODEL
# ============================================================================

print(f"\n{'='*60}")
print("BASELINE MODEL EVALUATION")
print(f"{'='*60}")

# Predict on test set
y_pred_baseline = baseline_model.predict(X_test)
y_pred_baseline = np.argmax(y_pred_baseline, axis=-1)
y_test_labels = np.argmax(y_test, axis=-1)

# Flatten predictions and true labels (excluding padding)
def get_flat_labels(y_true, y_pred, tag2idx):
    true_labels = []
    pred_labels = []
    
    for i in range(len(y_true)):
        for j in range(len(y_true[i])):
            if y_true[i][j] != tag2idx['O'] or y_pred[i][j] != tag2idx['O']:
                true_labels.append(idx2tag[y_true[i][j]])
                pred_labels.append(idx2tag[y_pred[i][j]])
    
    return true_labels, pred_labels

true_labels_baseline, pred_labels_baseline = get_flat_labels(
    y_test_labels, y_pred_baseline, tag2idx
)

# Classification report
print("\nClassification Report (Baseline):")
print(classification_report(true_labels_baseline, pred_labels_baseline))

# F1 Score
f1_baseline = f1_score(true_labels_baseline, pred_labels_baseline, average='weighted')
print(f"\nWeighted F1-Score (Baseline): {f1_baseline:.4f}")


# ============================================================================
# STEP 9: ADVANCED MODEL (Bidirectional LSTM with Dropout)
# ============================================================================

print(f"\n{'='*60}")
print("BUILDING ADVANCED MODEL")
print(f"{'='*60}")

def create_advanced_model(n_words, n_tags, max_len):
    input_layer = Input(shape=(max_len,))
    model = Embedding(input_dim=n_words + 1, output_dim=100, input_length=max_len)(input_layer)
    model = Dropout(0.1)(model)
    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)
    model = Dropout(0.1)(model)
    output_layer = TimeDistributed(Dense(n_tags, activation='softmax'))(model)
    
    model = Model(input_layer, output_layer)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    return model

advanced_model = create_advanced_model(n_words, n_tags, max_len)
print("\nAdvanced Model Architecture:")
advanced_model.summary()

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train advanced model
print("\nTraining advanced model...")
history_advanced = advanced_model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=15,
    validation_data=(X_val, y_val),
    callbacks=[early_stopping],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_advanced.history['loss'], label='Training Loss')
plt.plot(history_advanced.history['val_loss'], label='Validation Loss')
plt.title('Advanced Model - Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history_advanced.history['accuracy'], label='Training Accuracy')
plt.plot(history_advanced.history['val_accuracy'], label='Validation Accuracy')
plt.title('Advanced Model - Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()


# ============================================================================
# STEP 10: EVALUATE ADVANCED MODEL
# ============================================================================

print(f"\n{'='*60}")
print("ADVANCED MODEL EVALUATION")
print(f"{'='*60}")

# Predict on test set
y_pred_advanced = advanced_model.predict(X_test)
y_pred_advanced = np.argmax(y_pred_advanced, axis=-1)

# Flatten predictions and true labels
true_labels_advanced, pred_labels_advanced = get_flat_labels(
    y_test_labels, y_pred_advanced, tag2idx
)

# Classification report
print("\nClassification Report (Advanced):")
print(classification_report(true_labels_advanced, pred_labels_advanced))

# F1 Score
f1_advanced = f1_score(true_labels_advanced, pred_labels_advanced, average='weighted')
print(f"\nWeighted F1-Score (Advanced): {f1_advanced:.4f}")


# ============================================================================
# STEP 11: MODEL COMPARISON
# ============================================================================

print(f"\n{'='*60}")
print("MODEL COMPARISON")
print(f"{'='*60}")

comparison_df = pd.DataFrame({
    'Model': ['Baseline (LSTM)', 'Advanced (BiLSTM)'],
    'F1-Score': [f1_baseline, f1_advanced],
    'Improvement': [0, ((f1_advanced - f1_baseline) / f1_baseline * 100)]
})

print("\n", comparison_df.to_string(index=False))

# Visualization
plt.figure(figsize=(10, 6))
models = ['Baseline\n(LSTM)', 'Advanced\n(BiLSTM)']
scores = [f1_baseline, f1_advanced]
colors = ['skyblue', 'lightgreen']

bars = plt.bar(models, scores, color=colors, edgecolor='black', linewidth=2)
plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')
plt.ylabel('Weighted F1-Score', fontsize=12)
plt.ylim(0.8, 1.0)
plt.grid(axis='y', alpha=0.3)

# Add value labels on bars
for bar, score in zip(bars, scores):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{score:.4f}',
             ha='center', va='bottom', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()


# ============================================================================
# STEP 12: PREDICTION FUNCTION
# ============================================================================

def predict_ner(sentence, model, word2idx, idx2tag, max_len):
    """
    Predict NER tags for a given sentence
    """
    # Tokenize sentence
    words = sentence.split()
    
    # Convert to indices
    x = [word2idx.get(w, word2idx['UNK']) for w in words]
    
    # Pad sequence
    x = pad_sequences(maxlen=max_len, sequences=[x], padding='post', 
                      value=word2idx['ENDPAD'])
    
    # Predict
    pred = model.predict(x, verbose=0)
    pred = np.argmax(pred, axis=-1)[0]
    
    # Convert indices to tags
    predicted_tags = [idx2tag[i] for i in pred[:len(words)]]
    
    return list(zip(words, predicted_tags))


# ============================================================================
# STEP 13: TEST WITH EXAMPLES
# ============================================================================

print(f"\n{'='*60}")
print("TESTING WITH EXAMPLE SENTENCES")
print(f"{'='*60}")

test_sentences = [
    "John lives in New York",
    "Apple Inc is located in California",
    "Barack Obama was the president of United States",
    "Google and Microsoft are tech companies"
]

for sentence in test_sentences:
    print(f"\nSentence: {sentence}")
    result = predict_ner(sentence, advanced_model, word2idx, idx2tag, max_len)
    print("Predictions:")
    for word, tag in result:
        if tag != 'O':
            print(f"  {word} -> {tag}")


# ============================================================================
# STEP 14: SAVE MODEL
# ============================================================================

print(f"\n{'='*60}")
print("SAVING MODEL")
print(f"{'='*60}")

# Save the advanced model
advanced_model.save('ner_bilstm_model.h5')
print("Model saved as 'ner_bilstm_model.h5'")

# Save word and tag mappings
import pickle

with open('word2idx.pkl', 'wb') as f:
    pickle.dump(word2idx, f)

with open('tag2idx.pkl', 'wb') as f:
    pickle.dump(tag2idx, f)

with open('idx2tag.pkl', 'wb') as f:
    pickle.dump(idx2tag, f)

print("Mappings saved successfully!")


# ============================================================================
# STEP 15: SUMMARY AND RECOMMENDATIONS
# ============================================================================

print(f"\n{'='*60}")
print("PROJECT SUMMARY")
print(f"{'='*60}")

print(f"""
SHORTCOMINGS OF BASELINE MODEL:
1. Single directional LSTM - only looks at past context
2. No regularization - prone to overfitting
3. Lower embedding dimension - less expressive
4. Cannot capture bidirectional dependencies

IMPROVEMENTS IN ADVANCED MODEL:
1. Bidirectional LSTM - looks at both past and future context
2. Dropout layers - prevents overfitting
3. Higher embedding dimension - better word representations
4. Early stopping - optimal training duration

FUTURE SCOPE FOR OPTIMIZATION:
1. Use pre-trained word embeddings (GloVe, Word2Vec, FastText)
2. Implement CRF (Conditional Random Fields) layer for better tag sequence
3. Use Transformer-based models (BERT, RoBERTa) for state-of-the-art results
4. Add character-level embeddings to handle OOV words better
5. Implement attention mechanism
6. Use data augmentation techniques
7. Ensemble multiple models
8. Fine-tune on domain-specific data

MODEL PERFORMANCE:
- Baseline F1-Score: {f1_baseline:.4f}
- Advanced F1-Score: {f1_advanced:.4f}
- Improvement: {((f1_advanced - f1_baseline) / f1_baseline * 100):.2f}%
""")
